{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf097b3",
   "metadata": {},
   "source": [
    "<h1><center>Wrangle Report</center></h1>\n",
    "<center>By</center>\n",
    "<h3><center>Nishanth Iruthayaraj</center></h3>\n",
    "\n",
    "###  Introduction\n",
    "<br>\n",
    "<div style=\"text-align: justify\">\n",
    "The real-world data rarely comes clean. The process of assessing and cleaning this data is known as data wrangling. Therefore, there is always a requirment to wrangle this data in order to use them for further analyses and development. In this wrangle report, we describe the wrangle effort taken to gather, asses and clean the data. The data we used in this project for wrangling is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. This wrangle report documents the three significant process of data wrangling namely, Gathering Data, Assessing Data and Cleaning Data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20711cd",
   "metadata": {},
   "source": [
    "### 1. Gathering Data\n",
    "<br>\n",
    "<div style=\"text-align: justify\">\n",
    "The data gathering is the process to gather data from a variety of sources and in a variety of formats. In our project, we have three different data sources. <br>\n",
    "<br>\n",
    "           &nbsp; <b>I. WeRateDogs Twitter Archive:</b> This data has been provided by Udacity and we can directly download this file manually and read using 'Pandas' framework.<br>\n",
    "           &nbsp; <b>II. Tweet Image Predictions:</b> This data file is hosted on Udacity's servers and we can download it  programmatically using the 'Requests' library and the following URL: <mark>https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv</mark> <br>   \n",
    "           &nbsp; <b>III. Data from Twitter API: </b> We can download the additional information missing from the Twitter Archive data like each tweet's retweet count and favorite (\"like\") count and other interesting informations. I downloaded the tweet's JSON file programmatically using the 'Request' library and then extracted the necessary informations from the text file. \n",
    "    <div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48774a33",
   "metadata": {},
   "source": [
    "### 2. Assessing Data\n",
    "<br>\n",
    "<div style=\"text-align: justify\"> \n",
    "After gathering all three pieces of data as described in the Gathering Data section, we assess them visually and programmatically for quality and tidiness issues. The quality issues that I found are described below for each datasets\n",
    "<div>\n",
    "\n",
    "#### Quality Issues:\n",
    "<div style=\"text-align: justify\"> \n",
    "  \n",
    "##### twitter_archive_table\n",
    "- we need only the original tweets. So the records in in_reply_to_status and retweeted_status column can be removed \n",
    "- columns related to reply_to_status and retweeted_status should be removed to avoid NaN data types issues \n",
    "- datatype of timestamp is a string, should be change to datatime. Also +0000 at the end of timestamp should be removed\n",
    "- the source column doesn't not give the proper readable information for the users to read easily \n",
    "- datatype of rating_numerator and rating_denominator are int. The rating can also be in float based on the values in the text column\n",
    "- the some name of the dogs are `a` and `O`. One more letter can be added to make meaning to the name\n",
    "\n",
    "##### image_predictions_table\n",
    "- namings are not in identical format in p1, p2, and p3 columns\n",
    "- we need only images with original rating, but there are duplicates images. It should be removed\n",
    "- the preodictions are not only related to animals. There are various wrong predictions related to objects, velegatbles, etc., \n",
    "- There are many wrong predictions in p1, p2 and p3. We need confident prediction model. \n",
    "\n",
    "##### tweet_df_table\n",
    "- there should be relation with the tweet_archive table\n",
    "<div>\n",
    "\n",
    "#### Tidiness Issues:\n",
    "<div style=\"text-align: justify\"> \n",
    "    \n",
    "##### twitter_archive_table\n",
    "- the doggo, floofer, pupper and puppo define only the different stages of dogs. So they can be changed to single column\n",
    "- the tweet_id and retweet_count are related values in the tweet_archive table, so these 3 tables can be combined\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7607b2a2",
   "metadata": {},
   "source": [
    "### 3. Cleaning Data\n",
    "In this section, I clean the issues mentioned in the Assessing Data section before. Also each of the step may contain multiple **Define**, **Code** and **Test** process whenever is required during the cleaning process. The below are the definitions for cleaning the quality and tidiness issues mentioned earlier. The Code and Test are proven programmatically. \n",
    "\n",
    "##### Define, Code and Test:\n",
    "\n",
    "##### Cleaning Quality Issues:\n",
    "\n",
    "- I use `pd.isnull()` to remove entries other than NaN in in_reply_to_status_id and retweeted_status_id\n",
    "- We use `df.drop(columns)` to remove the columns related to reply and retweet\n",
    "- First slice the ' +0000' from the string using `.str[ ]` and use `.astype('datetime64[ns]')` to change into datetime datatype\n",
    "- We use `.replace()` to make the source understandable with suitable values\n",
    "- First we check the datatype of rating in the text column and if there is a decimal rating in it, we use `.astype(float)` to convert the datatype of rating numerator and denominator to float\n",
    "- We find the name of the dogs with single letter and replace all of them with user defined character using `df.replace()`\n",
    "- We use `str.title()` to make all the names of the prediction look in identical format\n",
    "- We use `.index()` and `.duplicated()` to find the indices of duplicated image url and use `.drop()` to remove the duplicated values\n",
    "\n",
    "##### Cleaning Tidiness Issues:\n",
    "- We use the `lambda` function to add the required columns by dropping the NaN values into new common column dog_stage. Then we fill the empty value in the new column with NaN again. Finally drop those 4 columns\n",
    "- We use `.merge()` to combine the tweet_archive and img_predictions tables first. Then the combined table can be merged again with the tweet_df table. We should also remove the indexes where the jpg_url column has NaN values to get the origial tweets\n",
    "\n",
    "*After completing the clearning process, we can reassess and verify the data again if needed and then store the master dataset as csv file for later usage.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
